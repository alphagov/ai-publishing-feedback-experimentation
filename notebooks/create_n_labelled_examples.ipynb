{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from src.utils.query_bigquery import query_bigquery\n",
    "from src.utils.sample import get_stratified_sample\n",
    "from src.utils.jsonify_data import jsonify_data\n",
    "from src.utils.async_call_openai import gather_responses\n",
    "from src.utils.write_to_bigquery import write_to_bigquery\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "FEEDBACK_PROJECT_ID = os.getenv(\"FEEDBACK_PROJECT_ID\")\n",
    "PUBLISHING_PROJECT_ID = os.getenv(\"PUBLISHING_PROJECT_ID\")\n",
    "FEEDBACK_TABLE = os.getenv(\"FEEDBACK_TABLE\")\n",
    "PUBLISHING_TABLE = os.getenv(\"PUBLISHING_TABLE\")\n",
    "LABELLED_FEEDBACK_TABLE = os.getenv(\"LABELLED_FEEDBACK_TABLE\")\n",
    "OPENAI_LABEL_FEEDBACK_TABLE = os.getenv(\"OPENAI_LABELLED_FEEDBACK_TABLE\")\n",
    "LABELLED_FEEDBACK_DATASET = os.getenv(\"LABELLED_FEEDBACK_DATASET\")\n",
    "OPENAI_LABEL_FEEDBACK_TABLE_FOR_REVIEW = os.getenv(\n",
    "    \"OPENAI_LABEL_FEEDBACK_TABLE_FOR_REVIEW\"\n",
    ")\n",
    "OLD_OPENAI_LABEL_FEEDBACK_TABLE_FOR_REVIEW = os.getenv(\n",
    "    \"OLD_OPENAI_LABEL_FEEDBACK_TABLE_FOR_REVIEW\"\n",
    ")\n",
    "EVALUATION_DATASET = os.getenv(\"EVALUATION_DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query BQ to pull the human labelled feedback data\n",
    "query_read = \"\"\"\n",
    "SELECT * FROM  @feedback_sample_table\n",
    "\"\"\"\n",
    "query_read = query_read.replace(\"@feedback_sample_table\", str(LABELLED_FEEDBACK_TABLE))\n",
    "\n",
    "# Call the function to execute the query\n",
    "labelled_sample = query_bigquery(\n",
    "    PUBLISHING_PROJECT_ID,\n",
    "    LABELLED_FEEDBACK_DATASET,\n",
    "    query_read,\n",
    ")\n",
    "\n",
    "# Get a stratified sample of the labelled feedback data\n",
    "stratified_sample = get_stratified_sample(\n",
    "    records=labelled_sample,\n",
    "    total_sample_size=20,\n",
    "    id_key=\"feedback_record_id\",\n",
    "    label_key=\"labels\",\n",
    ")\n",
    "\n",
    "stratified_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will label new records. Change the LIMIT to label more records.\n",
    "unlabelled_data_query = \"\"\"\n",
    "SELECT\n",
    "          feedback_record_id,\n",
    "          STRING_AGG(response_value, ' '\n",
    "          ORDER BY\n",
    "            created) AS concatenated_response_value,\n",
    "            rand() as r\n",
    "        FROM\n",
    "          @publishing_table\n",
    "          WHERE DATE(created) >= \"2024-01-01\"\n",
    "        GROUP BY\n",
    "          feedback_record_id\n",
    "        ORDER BY\n",
    "          r\n",
    "      LIMIT (10)\"\"\"\n",
    "\n",
    "unlabelled_data_query = unlabelled_data_query.replace(\n",
    "    \"@publishing_table\",\n",
    "    str(PUBLISHING_TABLE),\n",
    ")\n",
    "\n",
    "unlabelled_data = query_bigquery(\n",
    "    PUBLISHING_PROJECT_ID,\n",
    "    LABELLED_FEEDBACK_DATASET,\n",
    "    unlabelled_data_query,\n",
    ")\n",
    "\n",
    "# JSONify the data\n",
    "labelled_subs_json = jsonify_data(records=stratified_sample, labelled=True)\n",
    "new_subs_json = jsonify_data(records=unlabelled_data, labelled=False)\n",
    "\n",
    "# Call the OpenAI API to get the completions\n",
    "responses = await gather_responses(labelled_subs_json, new_subs_json, OPENAI_API_KEY)\n",
    "\n",
    "# Print the responses if there are any errors, note errors seem to only be around urgency that i've seen\n",
    "for item in responses:\n",
    "    if '\"urgency\":' not in item[\"open_labelled_records\"]:\n",
    "        print(item)\n",
    "\n",
    "# Check the costs of the queries\n",
    "prompt_tokens = [response[\"prompt_tokens\"] for response in responses]\n",
    "mean_prompt_tokens = np.mean(prompt_tokens)\n",
    "completion_tokens = [response[\"completion_tokens\"] for response in responses]\n",
    "mean_completion_tokens = np.mean(completion_tokens)\n",
    "print(\n",
    "    mean_prompt_tokens,\n",
    "    mean_completion_tokens,\n",
    "    f\"cost of prompts: ${2687 * (0.0005/1000):.5f} ||| cost of completions: ${37 * (0.0015 / 1000):.5f}\",\n",
    ")\n",
    "\n",
    "# Write the responses to the correct BigQuery table\n",
    "write_to_bigquery(\n",
    "    table_id=EVALUATION_DATASET,\n",
    "    responses=responses,\n",
    "    publishing_project_id=PUBLISHING_PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To query relabel the original 55 records that we used to test the quality of the promp use the following:\n",
    "unlabelled_data_query = \"\"\"\n",
    "SELECT\n",
    "  feedback_record_id,\n",
    "  STRING_AGG(response_value, ' '\n",
    "  ORDER BY\n",
    "    created) AS concatenated_response_value\n",
    "FROM\n",
    "  @publishing_table\n",
    "JOIN\n",
    "  @old_table\n",
    "ON\n",
    "  @publishing_table.feedback_record_id = @old_table.id\n",
    "GROUP BY\n",
    "  feedback_record_id\n",
    "  \"\"\"\n",
    "\n",
    "unlabelled_data_query = unlabelled_data_query.replace(\n",
    "    \"@publishing_table\",\n",
    "    str(PUBLISHING_TABLE),\n",
    ")\n",
    "unlabelled_data_query = unlabelled_data_query.replace(\n",
    "    \"@old_table\",\n",
    "    str(OLD_OPENAI_LABEL_FEEDBACK_TABLE_FOR_REVIEW),\n",
    ")\n",
    "\n",
    "unlabelled_data = query_bigquery(\n",
    "    PUBLISHING_PROJECT_ID,\n",
    "    LABELLED_FEEDBACK_DATASET,\n",
    "    unlabelled_data_query,\n",
    ")\n",
    "\n",
    "# JSONify the data\n",
    "labelled_subs_json = jsonify_data(records=stratified_sample, labelled=True)\n",
    "new_subs_json = jsonify_data(records=unlabelled_data, labelled=False)\n",
    "\n",
    "# Call the OpenAI API to get the completions\n",
    "responses = await gather_responses(labelled_subs_json, new_subs_json, OPENAI_API_KEY)\n",
    "\n",
    "# Print the responses if there are any errors, note errors seem to only be around urgency that i've seen\n",
    "for item in responses:\n",
    "    if '\"urgency\":' not in item[\"open_labelled_records\"]:\n",
    "        print(item)\n",
    "\n",
    "# Check the costs of the queries\n",
    "prompt_tokens = [response[\"prompt_tokens\"] for response in responses]\n",
    "mean_prompt_tokens = np.mean(prompt_tokens)\n",
    "completion_tokens = [response[\"completion_tokens\"] for response in responses]\n",
    "mean_completion_tokens = np.mean(completion_tokens)\n",
    "print(\n",
    "    mean_prompt_tokens,\n",
    "    mean_completion_tokens,\n",
    "    f\"cost of prompts: ${2687 * (0.0005/1000):.5f} ||| cost of completions: ${37 * (0.0015 / 1000):.5f}\",\n",
    ")\n",
    "\n",
    "# Write the responses to the correct BigQuery table\n",
    "write_to_bigquery(\n",
    "    table_id=OPENAI_LABEL_FEEDBACK_TABLE_FOR_REVIEW,\n",
    "    responses=responses,\n",
    "    publishing_project_id=PUBLISHING_PROJECT_ID,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
