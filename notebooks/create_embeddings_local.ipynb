{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch sentence-transformers python-dotenv google-cloud-bigquery -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PUBLISHING_PROJECT_ID = os.getenv(\"PUBLISHING_PROJECT_ID\")\n",
    "FEEDBACK_PROJECT_ID = os.getenv(\"FEEDBACK_PROJECT_ID\")\n",
    "FEEDBACK_TABLE = os.getenv(\"FEEDBACK_TABLE\")\n",
    "\n",
    "os.environ[\n",
    "    \"TOKENIZERS_PARALLELISM\"\n",
    "] = \"true\"  # Mostly set this to supress warnings but also to speed up tokenization; might cause issues with multiprocessing. If so set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "    print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "    print(\"__CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\n",
    "        \"__CUDA Device Total Memory [GB]:\",\n",
    "        torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "    )\n",
    "else:\n",
    "    print(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feedback_records(date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts feedback records from BigQuery\n",
    "    :param date: date from which to extract feedback records from\n",
    "    :return: DataFrame containing feedback records\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=FEEDBACK_PROJECT_ID)\n",
    "    query = \"\"\"\n",
    "      WITH CTE AS (\n",
    "          SELECT\n",
    "        feedback_record_id,\n",
    "        response_value\n",
    "      FROM\n",
    "        @feedback_table\n",
    "      WHERE\n",
    "        DATE(created) >= '@date'\n",
    "        AND response_type = 'text'\n",
    "        AND response_value IS NOT NULL\n",
    "        AND spam_classification = 'not spam'\n",
    "        AND ( prompt_value LIKE '%Please do not include personal or financial information, eg your National Insurance number or credit card details.%'\n",
    "          OR prompt_value LIKE 'what_doing'\n",
    "          OR prompt_value LIKE 'what_wrong'\n",
    "          OR prompt_value LIKE 'description'\n",
    "          OR prompt_value LIKE 'details' )\n",
    "        AND TYPE NOT IN ('ServiceFeedback',\n",
    "          'AggregatedServiceFeedback')\n",
    "    )\n",
    "      SELECT feedback_record_id,\n",
    "      REGEXP_REPLACE(STRING_AGG(response_value, '. '), r'\\.\\.', '.') AS response_value\n",
    "    FROM\n",
    "      CTE\n",
    "    GROUP BY\n",
    "      feedback_record_id\n",
    "    ORDER BY created\n",
    "  \"\"\"\n",
    "    query = query.replace(\"@feedback_table\", str(FEEDBACK_TABLE))\n",
    "    query = query.replace(\"@date\", date)\n",
    "    query_job = client.query(query=query)\n",
    "    result = query_job.result()\n",
    "    df = result.to_dataframe()\n",
    "    filtered_df = df.loc[df[\"response_value\"].str.strip().str.len() > 0]\n",
    "    print(f\"Num records in df: {len(filtered_df)} with cols {filtered_df.columns}\")\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(\n",
    "    data: pd.DataFrame,\n",
    "    default_model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get embeddings for a given text\n",
    "    :param text: str\n",
    "    :param default_model: str\n",
    "    :return: np.array\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SentenceTransformer(default_model, device=device)\n",
    "    response_value_list = data[\"response_value\"].tolist()\n",
    "    feedback_record_id_list = data[\"feedback_record_id\"].to_list()\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = model.encode(response_value_list)\n",
    "    print(f\"Embeddings len {len(embeddings)}\")\n",
    "    output_df = pd.DataFrame(\n",
    "        list(zip(feedback_record_id_list, embeddings)),\n",
    "        columns=[\"feedback_record_id\", \"embeddings\"],\n",
    "    )\n",
    "    print(\"Generated embeddings\")\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_embeddings_to_bigquery(data: pd.DataFrame, output_prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Save embedding DataFrame to BigQuery\n",
    "    :param data: DataFrame to save - in this instance we want to save chunks of feedback text and their embeddings, hence the WRITE_APPEND disposition\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    table_id = f\"{PUBLISHING_PROJECT_ID}.embeddings.{output_prefix}_feedback_embeddings\"\n",
    "    print(table_id)\n",
    "    client = bigquery.Client(project=PUBLISHING_PROJECT_ID)\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"feedback_record_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"response_value\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"embeddings\", \"FLOAT\", mode=\"REPEATED\"),\n",
    "        ],\n",
    "        write_disposition=\"WRITE_APPEND\"\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(data, table_id, job_config=job_config)\n",
    "    job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_null_responses(df: pd.DataFrame) -> object:\n",
    "    return df.isnull().sum()\n",
    "\n",
    "\n",
    "def check_df_len(df: pd.DataFrame) -> int:\n",
    "    return len(df)\n",
    "\n",
    "\n",
    "def check_len_embeddings(df: pd.DataFrame) -> float:\n",
    "    return df[\"embeddings\"].apply(lambda x: x.shape[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_and_embed_docs(data: pd.DataFrame, N: int, text_column: str) -> None:\n",
    "    \"\"\"\n",
    "    Split DataFrame into N chunks and apply function to each chunk\n",
    "    :param data: DataFrame to split\n",
    "    :param N: number of chunks to split DataFrame into\n",
    "    :param text_column: column containing text to apply function to\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    split_dfs = np.array_split(data, N)\n",
    "    for i, part in enumerate(split_dfs):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            chunk = pd.DataFrame(part)\n",
    "            print(\"Embedding data...\")\n",
    "            result = get_embeddings(data=chunk)\n",
    "            # print(f\"Number of empty responses: {check_empty_responses(result)}\")\n",
    "            print(f\"Number of null responses: {check_null_responses(result)}\")\n",
    "            print(f\"Number of responses: {check_df_len(result)}\")\n",
    "            print(f\"Number of embeddings: {check_len_embeddings(result)}\")\n",
    "            output_chunk = pd.merge(chunk, result, on=\"feedback_record_id\", how=\"left\")\n",
    "            print(f\"Created embeddings for chunk {i}\")\n",
    "            save_embeddings_to_bigquery(data=output_chunk, output_prefix=\"re_concat_mpnetv2\")\n",
    "            print(\"Embeddings saved to BQ\")\n",
    "            end = time.time()\n",
    "            print(f\"Time taken: {end - start}\")\n",
    "            print(output_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Chunk: {part}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test access to BQ\n",
    "client = bigquery.Client(project=PUBLISHING_PROJECT_ID)\n",
    "dataset = client.get_dataset(\"embeddings\")  # Make an API request.\n",
    "\n",
    "full_dataset_id = \"{}.{}\".format(dataset.project, dataset.dataset_id)\n",
    "friendly_name = dataset.friendly_name\n",
    "print(\n",
    "    \"Got dataset '{}' with friendly_name '{}'.\".format(\n",
    "        full_dataset_id, friendly_name\n",
    "    )\n",
    ")\n",
    "\n",
    "# View dataset properties.\n",
    "print(\"Description: {}\".format(dataset.description))\n",
    "print(\"Labels:\")\n",
    "labels = dataset.labels\n",
    "if labels:\n",
    "    for label, value in labels.items():\n",
    "        print(\"\\t{}: {}\".format(label, value))\n",
    "else:\n",
    "    print(\"\\tDataset has no labels defined.\")\n",
    "\n",
    "# View tables in dataset.\n",
    "print(\"Tables:\")\n",
    "tables = list(client.list_tables(dataset))  # Make an API request(s).\n",
    "if tables:\n",
    "    for table in tables:\n",
    "        print(\"\\t{}\".format(table.table_id))\n",
    "else:\n",
    "    print(\"\\tThis dataset does not contain any tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = get_feedback_records(\"2023-08-01\")\n",
    "split_and_embed_docs(df, N=20, text_column=\"response_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
