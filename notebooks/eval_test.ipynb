{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from src.collection.query_collection import (\n",
    "    filter_search,\n",
    "    get_semantically_similar_results,\n",
    ")\n",
    "from src.utils.utils import load_qdrant_client, load_config\n",
    "from src.utils.utils import load_model\n",
    "from src.collection.evaluate_collection import (\n",
    "    calculate_precision,\n",
    "    calculate_recall,\n",
    "    calculate_f1_score,\n",
    "    calculate_f2_score,\n",
    "    get_unique_labels,\n",
    "    get_data_for_evaluation,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\")\n",
    "QDRANT_PORT = os.getenv(\"QDRANT_PORT\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\")\n",
    "HF_MODEL_NAME = os.getenv(\"HF_MODEL_NAME\")\n",
    "PUBLISHING_PROJECT_ID = os.getenv(\"PUBLISHING_PROJECT_ID\")\n",
    "EVALUATION_TABLE = os.getenv(\"EVALUATION_TABLE\")\n",
    "EVALUATION_TABLE = f\"`{EVALUATION_TABLE}`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"../.config/config.json\")\n",
    "similarity_threshold = float(config.get(\"similarity_threshold_1\"))\n",
    "\n",
    "with open(\"../data/regex_ids.pkl\", \"rb\") as f:\n",
    "    regex_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = load_qdrant_client(QDRANT_HOST, port=QDRANT_PORT)\n",
    "model = load_model(HF_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(\"applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_semantically_similar_results(\n",
    "    client=qdrant,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query_embedding=query_embedding,\n",
    "    score_threshold=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ids = [str(result.id) for result in results]\n",
    "result_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_ids = regex_ids[\"application\"]\n",
    "apps_ids = regex_ids[\"applications\"]\n",
    "\n",
    "print(f\"intersection: {sorted(list(set(app_ids) & set(apps_ids)))}\")\n",
    "print(f\"apps_ids: {sorted(apps_ids)}\")\n",
    "print(f\"count of app_ids: {len(app_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = calculate_precision(result_ids, apps_ids)\n",
    "recall = calculate_recall(result_ids, apps_ids)\n",
    "f1_score = calculate_f1_score(precision, recall)\n",
    "f2_score = calculate_f2_score(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want high recall and we don't particularly mind if precision is low\n",
    "# because it just means that we are recommending more records than necessary\n",
    "# but we're including all the relevant records in our recommendations\n",
    "\n",
    "print(\n",
    "    f\"precision: {precision}\"\n",
    ")  # low precision = high fals positives (to be expected with low ANN similarity)\n",
    "print(\n",
    "    f\"recall: {recall}\"\n",
    ")  # high recall = low false negatives (to be expected with low ANN similarity)\n",
    "print(f\"f1_score: {f1_score}\")  # low f1 score = low precision\n",
    "print(f\"f2_score: {f2_score}\")  # low f2 score = low recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_for_evaluation(\n",
    "    project_id=PUBLISHING_PROJECT_ID,\n",
    "    evaluation_table=EVALUATION_TABLE,\n",
    ")\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = get_unique_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(unique_label, regex_ids, model, client, similarity_threshold):\n",
    "    # Get the count of records from the regex counts\n",
    "    relevant_records = regex_ids[unique_label]\n",
    "\n",
    "    # Embed the label\n",
    "    query_embedding = model.encode(unique_label)\n",
    "\n",
    "    # Retrieve the top K results for the label\n",
    "    try:\n",
    "        results = get_semantically_similar_results(\n",
    "            client=client,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_embedding=query_embedding,\n",
    "            score_threshold=similarity_threshold,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"get_semantically_similar_results error: {e}\")\n",
    "        pass\n",
    "\n",
    "    result_ids = [str(result.id) for result in results]\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = calculate_precision(result_ids, relevant_records)\n",
    "    recall = calculate_recall(result_ids, relevant_records)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over unique labels and similarity thresholds\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "for unique_label in unique_labels:\n",
    "    for threshold in np.arange(0, 1.1, 0.1):\n",
    "        precision, recall = calculate_metrics(\n",
    "            unique_label=unique_label,\n",
    "            regex_ids=regex_ids,\n",
    "            model=model,\n",
    "            client=qdrant,\n",
    "            similarity_threshold=threshold,\n",
    "        )\n",
    "        precision_values.append({unique_label: {threshold: precision}})\n",
    "        recall_values.append({unique_label: {threshold: recall}})\n",
    "\n",
    "# 33 mins to run all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle precision and recall values\n",
    "# with open(\"../data/precision_values.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(precision_values, f)\n",
    "\n",
    "# with open(\"../data/recall_values.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(recall_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_values(data_list):\n",
    "    # Dictionary to hold cumulative sums and counts for each test\n",
    "    sums_counts = defaultdict(lambda: {\"sum\": 0, \"count\": 0})\n",
    "\n",
    "    for item in data_list:\n",
    "        for _, values in item.items():\n",
    "            for threshold, value in values.items():\n",
    "                sums_counts[threshold][\"sum\"] += value\n",
    "                sums_counts[threshold][\"count\"] += 1\n",
    "\n",
    "    # Calculate mean for each test\n",
    "    mean_values = {\n",
    "        test: info[\"sum\"] / info[\"count\"] for test, info in sums_counts.items()\n",
    "    }\n",
    "    return mean_values\n",
    "\n",
    "\n",
    "# Calculate and print the mean values\n",
    "mean_precision_values = calculate_mean_values(precision_values)\n",
    "mean_recall_values = calculate_mean_values(recall_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = mean_precision_values\n",
    "recall_data = mean_recall_values\n",
    "\n",
    "x = list(data.keys())\n",
    "y = list(data.values())\n",
    "recall_x = list(recall_data.keys())\n",
    "recall_y = list(recall_data.values())\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(recall_x, recall_y)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Precision/Recall\")\n",
    "plt.title(\"Precision and Recall vs Threshold\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend([\"Precision\", \"Recall\"])\n",
    "\n",
    "# Remove top and right-hand side borders\n",
    "plt.gca().spines[\"top\"].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Add thin grey gridlines with an opacity of 0.3\n",
    "plt.grid(color=\"grey\", linestyle=\":\", linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Change x-axis labels to increment by 0.1\n",
    "plt.xticks([i / 10 for i in range(11)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for precision and recall\n",
    "with open(\"../data/precision_values.pkl\", \"rb\") as f:\n",
    "    precision_values = pickle.load(f)\n",
    "precision_values = [list(item.values())[0] for item in precision_values]\n",
    "rounded_precision_values = [\n",
    "    {round(key, 2): value for key, value in item.items()} for item in precision_values\n",
    "]\n",
    "rounded_precision_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_values(data, input_threshold=0.0):\n",
    "    threshold_list = []\n",
    "    for item in data:\n",
    "        for threshold, value in item.items():\n",
    "            if threshold == input_threshold:\n",
    "                threshold_list.append(value)\n",
    "    return threshold_list\n",
    "\n",
    "\n",
    "threshold_list = get_threshold_values(rounded_precision_values, input_threshold=0.0)\n",
    "\n",
    "plotting_values = {}\n",
    "\n",
    "for i in np.arange(0, 1.1, 0.1):\n",
    "    threshold_list = get_threshold_values(rounded_precision_values, input_threshold=i)\n",
    "    plotting_values[i] = threshold_list\n",
    "\n",
    "meta_list = []\n",
    "\n",
    "for threshold_value in np.arange(0, 0.3, 0.1):\n",
    "    threshold_list = get_threshold_values(\n",
    "        rounded_precision_values, input_threshold=threshold_value\n",
    "    )\n",
    "    meta_list.append(\n",
    "        {\n",
    "            \"threshold\": threshold_value,\n",
    "            \"mean\": np.mean(threshold_list),\n",
    "            \"median\": np.median(threshold_list),\n",
    "            \"std\": np.std(threshold_list),\n",
    "            \"min\": np.min(threshold_list),\n",
    "            \"max\": np.max(threshold_list),\n",
    "        }\n",
    "    )\n",
    "\n",
    "plotting_values\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.boxplot(plotting_values.values())\n",
    "# ax.set_xticklabels(data.keys())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "\n",
    "fig = go.Figure()\n",
    "# Your data dictionary\n",
    "for item in plotting_values.items():\n",
    "    fig.add_trace(go.Box(y=item[1], name=f\"Threshold: {item[0]}\", orientation=\"h\"))\n",
    "\n",
    "plot(fig)\n",
    "\n",
    "\n",
    "# Create a list of Box objects for Plotly, one for each key in the dictionary\n",
    "# box_plots = [\n",
    "#     go.Box(\n",
    "#         y=values,  # Assign values for the horizontal box plot\n",
    "#         name=str(key),  # Use the dictionary key as the name of the boxplot\n",
    "#         orientation=\"h\",  # 'h' for horizontal boxplot\n",
    "#     )\n",
    "#     for key, values in data_dict.items()\n",
    "# ]\n",
    "\n",
    "# Define layout options\n",
    "layout = go.Layout(\n",
    "    title=\"Horizontal Boxplots\", xaxis=dict(title=\"Values\"), yaxis=dict(title=\"Dataset\")\n",
    ")\n",
    "\n",
    "# Create the figure with data and layout\n",
    "# fig = go.Figure(data=box_plots, layout=layout)\n",
    "\n",
    "# Plot the figure\n",
    "# plot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "govuk-ai-publishing-feedback",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
